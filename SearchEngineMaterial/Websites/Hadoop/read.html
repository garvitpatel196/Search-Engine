<html>
<head>
  <title>READ DATA</title>
</head>
<body>
  <div>
    <h1>
      <center>


        READ DATA FROM HDFS USING API<br>
      </center>
    </h1>
  </div>
  <div>
    <h4>
     Now let’s understand complete end to end data read operation, as shown in the above figure the data read operation in hdfs is distributed, client reads the data parallely from datanodes, below are the steps by step explanation of data read cycle:

Step 1: Client opens the file it wishes to read by calling open() on the FileSystem object, which for HDFS is an instance of DistributedFileSystem.

Step 2: DistributedFileSystem calls the namenode using RPC to determine the locations of the blocks for the first few blocks in the file

For each block the namenode returns the addresses of the datanodes that have a copy of that block and datanode are sorted according to their proximity to the client.

Step 3: DistributedFileSystem return an FSDataInputStream to the client for it to read data from. FSDataInputStream in turns wraps the DFSInputStream which manages the datanode and namenode I/O

Client calls read() on the stream. DFSInputStream which has stored the datanode addresses then connects to the closest datanode for the first block in the file.

Step 4: Data is streamed from the datanode back to the client, which calls read() repeatedly on the stream. When the end of the block is reached DFSInputStream will close the connection to the datanode and then finds the best datanode for the next block.

Step 5: If the DFSInputStream encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. The DFSInputStream also verifies checksums for the data transferred to it from the datanode. If a corrupted block is found, it is reported to the namenode before the DFSInputStream attempts to read a replica of the block from another datanode.

Step 6: When the client has finished reading the data, it calls close() on the stream.

Failover process is same as data write operation, data read operation is also fault tolerant. To learn more features of hdfs follow this tutorial.

     </h4>
  </div>
  <div>
    <h2>
      Sample Code
    </h2>
    <h5>
leSystem fileSystem = FileSystem.get(conf);
Path path = new Path("/path/to/file.ext");
if (!fileSystem.exists(path)) {
System.out.println("File does not exists");
return;
}FSDataInputStream in = fileSystem.open(path);
int numBytes = 0;
while ((numBytes = in.read(b)) > 0) {
System.out.prinln((char)numBytes));// code to manipulate the data which is read}
in.close();
out.close();
fileSystem.close();    </h5>
  </div>
  
</body>
</html>
