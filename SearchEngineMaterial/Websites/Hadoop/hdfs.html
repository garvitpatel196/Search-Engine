<html>
<head>
  <title>HDFS</title>
</head>
<body>
  <div>
    <h1>
      <center>
        <a href="write.html">CLICK HERE TO DATA WRITE IN HDFS USING API</a><br>
        <a href="read.html">CLICK HERE TO READ DATA FROM HDFS USING API</a><br>
        HDFS<br>
      </center>
    </h1>
  </div>
  <div>
    <h4>
      The Hadoop distributed file system (HDFS) is a distributed, scalable, and portable file system written in Java for the Hadoop framework. Some consider HDFS to instead be a data store due to its lack of POSIX compliance and inability to be mounted,[62] but it does provide shell commands and Java API methods that are similar to other file systems.[63] A Hadoop cluster has nominally a single namenode plus a cluster of datanodes, although redundancy options are available for the namenode due to its criticality. Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses TCP/IP sockets for communication. Clients use remote procedure call (RPC) to communicate between each other.

HDFS stores large files (typically in the range of gigabytes to terabytes[64]) across multiple machines. It achieves reliability by replicating the data across multiple hosts, and hence theoretically does not require RAID storage on hosts (but to increase I/O performance some RAID configurations are still useful). With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully POSIX-compliant, because the requirements for a POSIX file-system differ from the target goals for a Hadoop application. The trade-off of not having a fully POSIX-compliant file-system is increased performance for data throughput and support for non-POSIX operations such as Append.[65]

HDFS added the high-availability capabilities, as announced for release 2.0 in May 2012,[66] letting the main metadata server (the NameNode) fail over manually to a backup. The project has also started developing automatic fail-over.

The HDFS file system includes a so-called secondary namenode, a misleading name that some might incorrectly interpret as a backup namenode for when the primary namenode goes offline. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple namespaces served by separate namenodes. Moreover, there are some issues in HDFS, namely, small file issue, scalability problem, Single Point of Failure (SPoF), and bottleneck in huge metadata request. An advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location. For example: if node A contains data (x,y,z) and node B contains data (a,b,c), the job tracker schedules node B to perform map or reduce tasks on (a,b,c) and node A would be scheduled to perform map or reduce tasks on (x,y,z). This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems, this advantage is not always available. This can have a significant impact on job-completion times, which has been demonstrated when running data-intensive jobs.[67]

HDFS was designed for mostly immutable files[65] and may not be suitable for systems requiring concurrent write-operations.

HDFS can be mounted directly with a Filesystem in Userspace (FUSE) virtual file system on Linux and some other Unix systems.

File access can be achieved through the native Java application programming interface (API), the Thrift API to generate a client in the language of the users' choosing (C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk, and OCaml), the command-line interface, browsed through the HDFS-UI Web application (webapp) over HTTP, or via 3rd-party network client libraries.[68]

HDFS is designed for portability across various hardware platforms and compatibility with a variety of underlying operating systems. The HDFS design introduces portability limitations that result in some performance bottlenecks, since the Java implementation can't use features that are exclusive to the platform on which HDFS is running.[69] Due to its widespread integration into enterprise-level infrastructures, monitoring HDFS performance at scale has become an increasingly important issue. Monitoring end-to-end performance requires tracking metrics from datanodes, namenodes, and the underlying operating system.[70] There are currently several monitoring platforms to track HDFS performance, including HortonWorks, Cloudera, and Datadog.


    </h4>
  </div>
  <div>
    <h2>
      DFS_Requirement
    </h2>
    <h5>
      The Hadoop Distributed File system (DFS) is a fault tolerant scalable distributed storage component of the Hadoop distributed high performance computing platform. The purpose of this document is to summarize the requirements Hadoop DFS should be targeted for, and to outline further development steps towards achieving this requirements.

The requirements are divided below into groups: reliability, scalability, functionality, performance, etc. listed in the order of their importance. The requirements are intended to reflect the scale and the nature of the distributed tasks that should run under the Hadoop platform. The prioritized list of projects presented in the last section is a cumulative digest of ideas proposed by Hadoop developers that we believe will lead to achieving formulated goals. As the next step we would like to socialize and finalize the list. An important step in this direction would be building a re-factoring procedure which would let us upgrade different components of DFS step by step keeping the system functional and backward compatible all the way through the process.

DFS Scale requirements:

Number of nodes – 10 thousand.
Total data size – 10 PB.
Assuming 10,000 nodes capable of storing 1TB each. This is an order of magnitude estimate. With 750GB disks becoming commodity we could reasonably expect to have to support 750GB*4/node = 3TB/node = 30PB total.
Number of files – 100 million.
If DFS data size is 1016 and the block size is 108 then under the assumption that each file has exactly one block we need to support 108 files.
On our current installation there is 32TB of data, using 55,000 files and folders. Scaling 32TB to 10PB under the assumption the average file size remains the same gives us an estimate of 18,000,000 files.
Number of concurrent clients – 100 thousand.
If on a 10,000 cluster each node has one task tracker running 4 tasks each according to current m/r defaults then we need to support 40,000 simultaneous clients.
Acceptable level of data loss – 1 hour.
Any data created or updated in DFS 1 hour ago or before is guaranteed to be recoverable in case of system failures.
Acceptable downtime level – 2 hours.
DFS failure requires manual system recovery. The system is guaranteed to be available again not later than 2 hours after the recovery start.
Feature requirements:

File Operations: open, create, close, read, append, truncate, delete, rename, undelete, get block locations, set replication, seek, tell
Exclusive and shared access for serial/random reads and appends.
Appenders should support flush() operation, which would flush buffer directly to DFS.
File system Operations: readdir, directory rename, statistics (nfiles, nbytes, nblocks)
Robust check-pointing and journaling
Metadata versioning for backward compatibility
Programming language agnostic client protocol
Topology awareness (rack awareness minimally) for smarter block placement
Automatic corruption detection and correction
Atomic append
Multiple appenders
Owners, permissions, quotas
List of projects:

Re-factoring. Develop abstractions for DFS components with each component represented by an interface, specifying its functionality and interaction with other components. With good abstractions, it should be easy to add new features without compromising reliability. The abstractions should be evaluated with required future features in mind. 
For example, data nodes might have a block transfer object, a block receive object, etc., with carefully defined behavior, coordinated by a top-level control structure, instead of the morass of methods in the data node at present.
(Reliability) Robust name node checkpointing and namespace edits logging. 
Currently the system is not restorable in case of name node hardware failure. 
DFS should store “image” and “edits” files on a local name node disk and replicate them on backup nodes using a simple streaming protocol. 
HADOOP-90 Done.
(Reliability) Define the startup process, what is done by each component, in which order. Introduce a concept of “safe mode”, which would not make any block replication/removal decisions or change the state of the namespace in any way. Name node stays in safe mode until a configurable number of nodes have been started and reported to the name node a configurable percentage of data blocks. 
HADOOP-306 Done.
(Reliability) The name node checkpoint should store a list of data nodes serving distinct data storages that ever reported to the name node. Namely, the following is stored for each data node in the cluster: 
<host:port; storageID; time of last heartbeat; user id>. 
Missing nodes should be reported in the DFS UI, and during the startup. See also 3.a. 
HADOOP-456, Done.
(Reliability) Nodes with read only disks should report the problem to the name node and shut themselves down if all their local disks are unavailable. 
HADOOP-163 Done.
(Specification) Define recovery/failover and software upgrade procedures.
The recovery of the cluster is manual; a document describing steps for the cluster safe recovery after a name node failure is desired.
Based on the recovery procedures estimate the downtime of the cluster when the name node fails.
A document is needed describing general procedures required to transition DFS from one software version to another. 
Hadoop_Upgrade, HADOOP-702 Done.
(Reliability) The name node should boost the priority of re-replicating blocks that are far from their replication target. If necessary it should delay requests for new blocks, opening files etc., in favor of re-replicating blocks that are close to being lost forever. 
HADOOP-659 Done.
(Functionality) Currently DFS supports exclusive on create only file appends. We need more general appends that would allow re-opening files for appending. Our plan is to implement it in two steps:
Exclusive appends. 
HADOOP-1700, HDFS-265 Done.
Concurrent appends.
(Functionality) Support for “truncate” operation. 
This is a new functionality that is not currently supported by DFS.
(Functionality) Configuration:
Accepting/rejecting rules for hosts and users based on regular expressions. The string that is matched against the regular expression should include the host, user, and cluster names. 
HADOOP-442 Done.
(Functionality) DFS browsing UI. 
Currently DFS has a rather primitive UI. 
The UI should
Let browse the file system going down to each file, each file block, and further down to the block replicas. 
HADOOP-347 , HADOOP-392 Done.
Report status of each directory, file, block, and block replica. 
HADOOP-347 Done.
Show list of data nodes, their status, and non-operational nodes (see 4). 
HADOOP-250 Done.
Show data node configuration and its extended status.
List data node blocks and file names they belong to.
Report the name node configuration parameters.
History of data node failures, restarts, etc.
(Scalability) Nodes with multiple disks should maintain local disks data distribution internally. 
HADOOP-64 Done.
(Scalability) Select-based communication for the DFS name node.
(Functionality) Currently, if we want to remove x nodes from the DFS cluster, we need to remove them at most two at a time, and wait until re-replication happens, and there's no feedback on that. It would be good to specify a list of nodes to remove, have their data re-replicated while they're still online, and get a confirmation on completion.
HADOOP-681 Done.
(Specification) Define invariants for read and append commands. A formalization of DFS consistency model with underlying assumptions and the result guarantees.
(Performance) Check sum data should not be stored as a separate DFS crc-file, but rather maintained by a data node per locally stored block copy. This will reduce name node operations and improve read data locality for maps
HADOOP-1134 Done.
CRC scanning. We should dedicate up to 1% of the disk bandwidth on a data node to reading back the blocks and validating their CRCs. The results should be logged and reported in the DFS UI
HADOOP-2012 Done.
(Performance) DFS should operate with constant size file blocks. 
Currently internally DFS supposes that blocks of the same file can have different sizes. In practice all of them except for the last one have the same size. The code could be optimized if the above assumption is removed. 
Each block can be of any size up to the file’s fixed block size. The DFS client provides an API to report gaps and/or an API option to skip gaps or see them as NULLs. The reporting is done at the data node level allowing us to remove all the size data & logic at the name node level.
(Performance) Client writes should flush directly to DFS based on the buffer size set at creation of the stream rather than collecting data in a temporary file on a local disk.
HADOOP-66 Done.
(Performance) Currently data nodes report the entire list of stored blocks to the name node once in an hour. Most of this information is redundant. Processing of large reports reduces the name node availability for application tasks. 
Possible solutions:
Data nodes report a portion (e.g. 20%, or bounded by the total size of transmitted data) of their blocks but (5 times) more often.
Data nodes report just the delta with the removed blocks being explicitly marked as such. 
On startup the name node restores its state from a checkpoint. The checkpoint stores information about files and their blocks, but not the block locations. The locations are restored from the data node reports. That is why, at startup data nodes need to report complete lists of stored blocks. Subsequent reports do not need to contain all blocks, just the ones that have been modified since the last report. 
Each data node reports its blocks in one hour intervals. In order to avoid traffic jams the name node receives reports from different data nodes at different randomized times. Thus, on e.g. a 600 node cluster the name node receives 10 reports per minute, meaning that the block list validation happens 10 times a minute. We think it is important to minimize the reporting data size mostly from the point of view of the receiver. 
The name node should have means to request complete reports from data nodes, which is required in case the name node restarts.
HDFS-395
(Performance) Fine grained name node synchronization. Rather than locking the whole name node for every namespace update, the name node should have only a few synchronized operations. These should be very efficient, not performing i/o and allocating few if any objects. This synchronous name node kernel should be well-defined, so that developers were aware of its boundaries.
HADOOP-814 Done.
(Performance) Compact name node data structure. In order to support large namespaces the name node should efficiently represent internal data, which particularly mean eliminating redundant block mappings. 
Currently DFS supports the following blocks mappings:
Block to data node map (FSNamesystem.blocksMap)
Data node to block map (FSNamesystem.datanodeMap)
INode to block map (INode.blocks)
Block to INode map (FSDirectory.activeBlocks)
HADOOP-1687 Done.
(Performance) Improved block allocation schema. 
Currently DFS randomly selects nodes from the set of data nodes that can fit the required amount of data (a block). 
Things we need:
Rack locality awareness. First replica is placed on the client’s local node, the second replica is placed on a node in the same rack as the client, and all other replicas are placed randomly on the nodes outside the rack.
HADOOP-692 Done.
Current replication policy is to place the first replica on the local node, to place the second replica on a remote rack, and to place the third replica on the same rack as the second one.
Nodes with high disk usage should be avoided for block placement.
Nodes with high workload should be avoided for block placement.
Distinguish between fast and slow nodes (performance– and communication–wise).
(Performance) Equalize disk space usage between the nodes. The name node should regularly analyze data node disk states, and re-replicate blocks if some of them are “unusually” low or high on storage.
HADOOP-1652 Done.
(Performance) Commands “open” and “list” should not return the entire list of block locations, but rather return a fixed number of initial blocks. Reads will fetch required block location when and if necessary.
HADOOP-894 Done.
(Performance) When the name node is busy it should reply to data nodes that they should retry reporting their blocks later. The data nodes should retry reporting the blocks in this case earlier than the regular report would occur.
(Functionality) Implement atomic append command, also known as “record append”.
(Functionality) Hadoop command shell should conform to the common shell conventions.
Hadoop commands should support common options like -D, -conf, -fs, etc. Each command at the same time can have its specific options.
Commons CLI should be used to implement the generic options parser and a specific command's option parsers.
Support for meta-characters and regular expressions in general for file names (cp, mv, rm, ls).
Interactive mode: ability to issue several commands in the same session, while keeping track of the context (such as pwd).
Scripts: the ability to execute several commands recorded in a text file.
(Interoperability) Slim client design. The majority of the current client logic should be placed into a data node, called “primary”, which controls the process of data transfer to other nodes, lease extension and confirmation or failure reporting if required. A thin client makes it easier to keep Java, C, etc. client implementations in sync. 
Currently the client plays the role of the primary node.
(Interoperability) Implement WebDav and NFS server mounting capabilities for DFS.</h5>
  </div>
  <div>
    <h2>
      HDFS Architecture
    </h2>
    <h5>
      NameNode and DataNodes

HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.

HDFS Architecture

The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case.

The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.

The File System Namespace

HDFS supports a traditional hierarchical file organization. A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file. HDFS supports user quotas and access permissions. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features.

The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the NameNode. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the replication factor of that file. This information is stored by the NameNode.

Data Replication

HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file.

All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.

An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once (except for appends and truncates) and have strictly one writer at any time.

The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.

HDFS DataNodes

    </h5>
  </div>
  </body>
</html>
