<html>
<head>
  <title>Data Write</title>
</head>
<body>
  <div>
    <h1>
      <center>

        Data Write In HDFS<br>
      </center>
    </h1>
  </div>
  <div>
    <h4>
     Now let’s understand complete end to end data write pipeline, as shown in the above figure the data write operation in hdfs is distributed, client copies the data distributedly on datanodes, below are the steps by step explanation of data write operation:

Step 1: The hdfs client sends create request on DistributedFileSystem APIs.

Step 2: DistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem’s namespace.

The namenode performs various checks to make sure that the file doesn’t already exist and that the client has the permissions to create the file. If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException.

Step 3: TheDistributedFileSystem returns an FSDataOutputStream for the client to start writing data to. As the client writes data, DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue. The data queue is consumed by the DataStreamer, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas.

Step 4: The list of datanodes form a pipeline, and here we’ll assume the replication level is three, so there are three nodes in the pipeline. TheDataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline.

Step 5: DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by the datanodes in the pipeline. Datanode sends the acknowledgement once required replicas are created (3 by default). Similarly all the blocks are stored and replicated on the different datanodes, the data blocks are copied in parallel

Step 6: When the client has finished writing data, it calls close() on the stream.

Step 7: This action flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete. The namenode already knows which blocks the file is made up of, so it only has to wait for blocks to be minimally replicated before returning successfully.

Now what happens when one of the machines i.e. part of the pipeline which has datanode process running fails. Hadoop has inbuilt functionality to handle this scenario (hdfs is fault tolerant). If a datanode fails while data is being written to it, then the following actions are taken, which are transparent to the client writing the data.

First, the pipeline is closed, and any packets in the ack queue are added to the front of the data queue so that datanode that are downstream from the failed node will not miss any packets.
The current block on the good datanode is given a new identity, which is communicated to the namenode, so that the partial block on the failed datanode will be deleted if the failed datanode recovers later on.
The failed datanode is removed from the pipeline, and the remainder of the block’s data is written to the two good datanodes in the pipeline.
The namenode notices that the block is under-replicated, and it arranges for a further replica to be created on another node. Subsequent blocks are then treated as normal.
It’s possible, but unlikely, that multiple datanodes fail while a block is being written. As long as dfs.replication.min replicas (which default to one) are written, the write will succeed, and the block will be asynchronously replicated across the cluster until its target replication factor is reached (dfs.replication, which defaults to three). HDFS is highly reliable, fault-tolerant, highly-available distributed file-system to learn more features of hdfs follow this comprehensive tutorial.    </h4>
  </div>
  <div>
    <h2>
      Sample Code
    </h2>
    <h5>
      FileSystem fileSystem = FileSystem.get(conf);
// Check if the file already exists
Path path = new Path("/path/to/file.ext");
if (fileSystem.exists(path)) {
System.out.println("File " + dest + " already exists");
return;
}
// Create a new file and write data to it.
FSDataOutputStream out = fileSystem.create(path);
InputStream in = new BufferedInputStream(new FileInputStream(
new File(source)));
 
byte[] b = new byte[1024];
int numBytes = 0;
while ((numBytes = in.read(b)) > 0) {
out.write(b, 0, numBytes);
}
// Close all the file descripters
in.close();
out.close();
fileSystem.close();    </h5>
  </div>
  </body>
</html>
